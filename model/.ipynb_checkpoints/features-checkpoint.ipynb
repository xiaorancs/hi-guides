{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_1 = pd.read_csv('../data/dataSet/df_train_1.csv')\n",
    "df_test_1 = pd.read_csv('../data/dataSet/df_test_1.csv')\n",
    "\n",
    "\n",
    "df_train_2 = pd.read_csv('../data/dataSet/df_train_2.csv')\n",
    "df_test_2 = pd.read_csv('../data/dataSet/df_test_2.csv')\n",
    "\n",
    "\n",
    "df_train_3 = pd.read_csv('../data/dataSet/df_train_3.csv')\n",
    "df_test_3 = pd.read_csv('../data/dataSet/df_test_3.csv')\n",
    "\n",
    "\n",
    "df_train_4 = pd.read_csv('../data/dataSet/df_train_4.csv')\n",
    "df_test_4 = pd.read_csv('../data/dataSet/df_test_4.csv')\n",
    "\n",
    "df_train_5 = pd.read_csv('../data/dataSet/df_train_5.csv')\n",
    "df_test_5 = pd.read_csv('../data/dataSet/df_test_5.csv')\n",
    "\n",
    "df_train_6 = pd.read_csv('../data/dataSet/df_train_6.csv')\n",
    "df_test_6 = pd.read_csv('../data/dataSet/df_test_6.csv')\n",
    "\n",
    "df_train_7 = pd.read_csv('../data/dataSet/df_train_7.csv')\n",
    "df_test_7 = pd.read_csv('../data/dataSet/df_test_7.csv')\n",
    "\n",
    "df_train_8 = pd.read_csv('../data/dataSet/df_train_8.csv')\n",
    "df_test_8 = pd.read_csv('../data/dataSet/df_test_8.csv')\n",
    "\n",
    "df_train_9 = pd.read_csv('../data/dataSet/df_train_9.csv')\n",
    "df_test_9 = pd.read_csv('../data/dataSet/df_test_9.csv')\n",
    "\n",
    "df_train_10 = pd.read_csv('../data/dataSet/df_train_10.csv')\n",
    "df_test_10 = pd.read_csv('../data/dataSet/df_test_10.csv')\n",
    "\n",
    "df_train_11 = pd.read_csv('../data/dataSet/df_train_11.csv')\n",
    "df_test_11 = pd.read_csv('../data/dataSet/df_test_11.csv')\n",
    "\n",
    "df_train_12 = pd.read_csv('../data/dataSet/df_train_12.csv')\n",
    "df_test_12 = pd.read_csv('../data/dataSet/df_test_12.csv')\n",
    "\n",
    "df_train_12_top110 = pd.read_csv('../data/dataSet/df_train_12_top110.csv')\n",
    "df_test_12_top110 = pd.read_csv('../data/dataSet/df_test_12_top110.csv')\n",
    "\n",
    "df_train_13 = pd.read_csv('../data/dataSet/df_train_13.csv')\n",
    "df_test_13 = pd.read_csv('../data/dataSet/df_test_13.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopfeatures = ['hasOrder','actionTypeIn24Avg','actionTypeIn14Avg',\n",
    "                'actionTime24CurLastDiff','actionTime59CurLastDiff','actionTimeLast0.042Days8Cnt',\n",
    "                'actionTimeLast0.042Days9Cnt','actionTimeLast0.125Days8Cnt','actionTimeLast0.125Days9Cnt',\n",
    "                'actionTimeLast0.25Days8Cnt','actionTimeLast0.25Days9Cnt','actionTimeLast0.5Days24Cnt',\n",
    "                'actionTimeLast0.5Days59Cnt','actionTimeLast0.5Days9Cnt','actionTimeLast15Days24Cnt',\n",
    "                'actionTimeLast15Days59Cnt','actionTimeLast1Days24Cnt','actionTimeLast1Days59Cnt',\n",
    "                'actionTimeLast1Days9Cnt','actionTimeLast3Days24Cnt','actionTimeLast3Days59Cnt',\n",
    "                'actionTimeLast3Days9Cnt','actionTimeLast5Days24Cnt','actionTimeLast5Days59Cnt',\n",
    "                'actionTimeLast7Days24Cnt','actionTimeLast7Days59Cnt','actionTimeLast5Days9Cnt',\n",
    "                 # df_xtrain_6\n",
    "                'actionTime24LastFirstDiffRate','actionType8Cnt','actionTypeIn14Cnt','actionTime24CurFirstDiff',\n",
    "                'actionType3Cnt_actionCnt_rate','actionTypeIn24Cnt','actionTime24LastFirstDiff',\n",
    "                'actionTypeIn29Cnt','actionTypeCnt','actionType4Cnt','actionType2Cnt',\n",
    "                'actionType3Cnt','actionTypeIn29Cnt_actionCnt_rate','actionTypeIn14Cnt_actionCnt_rate',\n",
    "                'actionType9Cnt','actionTypeIn14Avg','actionTypeIn59Avg','actionTypeIn29Avg','actionTypeIn24Avg',\n",
    "                # df_xtrain_7\n",
    "                'type4DiffTimeAvg','typeDiff8TimeMaxSubAvg','type4Last3DiffTime','type2Last2DiffTime',\n",
    "                'type9DiffTimeAvg','typeDiff2TimeMaxSubAvg','type7Last2DiffTime','typeDiff4TimeAvgSubMin',\n",
    "                'type8Last2DiffTime','type4Last2DiffTime','typeDiff7TimeMaxSubMin','typeDiff8TimeAvgSubMin',\n",
    "                'type3DiffTimeMax','type2DiffTimeAvg','type3Last2DiffTime','type2DiffTimeMin',\n",
    "                'typeDiff2TimeAvgSubMin','type8Last1DiffTime','type8DiffTimeAvg','type8DiffTimeMin',\n",
    "                'type3DiffTimeAvg','type2DiffTimeMax','type4DiffTimeMax','typeDiff3TimeMaxSubMin',\n",
    "                'typeDiff8TimeMaxSubMin','type8Last3DiffTime','typeDiff2TimeMaxSubMin','type7Last3DiffTime',\n",
    "                'typeDiff4TimeMaxSubMin','typeDiff7TimeAvgSubMin','type2Last3DiffTime','typeDiff3TimeAvgSubMin',\n",
    "                'type9DiffTimeMax','type2Last1DiffTime','type4Last1DiffTime','typeDiff9TimeAvgSubMin',\n",
    "                'type3DiffTimeMin','type3Last1DiffTime','type9Last2DiffTime','type3Last3DiffTime',\n",
    "                'type4DiffTimeMin','type9Last3DiffTime','type9DiffTimeMin','typeDiff9TimeMaxSubAvg',\n",
    "                'type9Last1DiffTime','typeDiff9TimeMaxSubMin',\n",
    "                # df_xtrain_8\n",
    "                'actionTimeLast0.042Days1Cnt','actionTimeLast15Days2Cnt','actionTimeLast15Days8Cnt',\n",
    "                'actionTimeLast0.5Days6Cnt','actionTimeLast0.25Days5Cnt','actionTimeLast0.25Days6Cnt',\n",
    "                 'actionTimeLast0.125Days1Cnt','actionTimeLast0.5Days1Cnt','actionTimeLast0.125Days59Cnt',\n",
    "                 'actionTimeLast15Days9Cnt','actionTimeLast5Days2Cnt','actionTimeLast7Days8Cnt',\n",
    "                 'actionTimeLast7Days7Cnt','actionTimeLast7Days3Cnt','actionTimeLast7Days4Cnt',\n",
    "                 'actionTimeLast5Days4Cnt','actionTimeLast3Days4Cnt','actionTimeLast7Days2Cnt',\n",
    "                 'actionTimeLast1Days8Cnt','actionTimeLast0.5Days7Cnt','actionTimeLast3Days3Cnt',\n",
    "                 'actionTimeLast3Days2Cnt','actionTimeLast1Days3Cnt','actionTimeLast0.125Days4Cnt',\n",
    "                 'actionTimeLast0.125Days7Cnt','actionTimeLast5Days8Cnt','actionTimeLast1Days4Cnt',\n",
    "                 'actionTimeLast0.042Days4Cnt','actionTimeLast0.25Days7Cnt','actionTimeLast3Days8Cnt',\n",
    "                 'actionTimeLast0.042Days3Cnt','actionTimeLast3Days7Cnt','actionTimeLast0.5Days8Cnt',\n",
    "                 'actionTimeLast5Days3Cnt','actionTimeLast0.125Days3Cnt','actionTimeLast1Days7Cnt',\n",
    "                 'actionTimeLast0.5Days3Cnt','actionTimeLast0.125Days24Cnt','actionTimeLast1Days2Cnt',\n",
    "                 'actionTimeLast0.125Days2Cnt','actionTimeLast0.25Days3Cnt','actionTimeLast5Days7Cnt',\n",
    "                 'actionTimeLast0.042Days2Cnt','actionTimeLast0.5Days4Cnt','actionTimeLast7Days9Cnt',\n",
    "                 'actionTimeLast0.5Days2Cnt','actionTimeLast5Days9Cnt','actionTimeLast0.25Days4Cnt',\n",
    "                 'actionTimeLast0.25Days2Cnt','actionTimeLast3Days9Cnt','actionTimeLast1Days9Cnt',\n",
    "                 'actionTimeLast0.25Days24Cnt','actionTimeLast0.125Days8Cnt','actionTimeLast0.125Days9Cnt',\n",
    "                 'actionTimeLast0.25Days59Cnt','actionTimeLast0.5Days59Cnt','actionTimeLast0.042Days9Cnt',\n",
    "                 'actionTimeLast0.042Days8Cnt','actionTimeLast0.25Days8Cnt','actionTimeLast1Days59Cnt',\n",
    "                # df_xtrain_10\n",
    "                'type7Last2Time','type2Last2Time','type9Last1Time','type8Last2Time','type2Last3Time',\n",
    "                'type7Last3Time','type3Last3Time','type4Last2Time','type4Last3Time','type3Last2Time',\n",
    "                'type8Last3Time','type9Last2Time','type9Last3Time']\n",
    "\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [df_train_1 ---> df_train_12]: 使用所有的12个文件， submission1_12.csv\n",
    "\n",
    "3. [df_train_1 ---> df_train_12_top110.csv] - stopfeature : submisson1_120top_stopf.csv\n",
    "\n",
    "4. [df_train_1 ---> df_train_12_top110.csv] + df_train_13(stack) - stopfeature : submission1_120top_stopf.csv\n",
    "\n",
    "5. [df_train_1 ---> df_train_6,] + df_train_12.csv - stopfeature submission1_7_12.csv\n",
    "\n",
    "6. [df_train_1 ---> df_train_6,] + df_train_12.csv + df_strain_13 - stopfeature  submission1_7_12_13.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDataSet1():\n",
    "    df_train = pd.read_csv('../data/train/orderFuture_train.csv')\n",
    "    df_test = pd.read_csv('../data/test/orderFuture_test.csv')\n",
    "\n",
    "    df_train = pd.merge(df_train,df_train_1,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_1,how='left',on='userid')\n",
    "   \n",
    "    df_train = pd.merge(df_train,df_train_2,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_2,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_3,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_3,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_4,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_4,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_5,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_5,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_6,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_6,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_7,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_7,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_8,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_8,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_9,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_9,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_10,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_10,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_11,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_11,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_12,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_12,how='left',on='userid')\n",
    "    \n",
    "    goodfeature = list(df_test.columns[1:])\n",
    "\n",
    "    for f in stopfeatures:\n",
    "        if f in goodfeature:\n",
    "            goodfeature.remove(f)\n",
    "    \n",
    "    # 设置特征数据，去除id数据，不能进行预测\n",
    "    features = goodfeature\n",
    "\n",
    "    label = 'orderType'\n",
    "    \n",
    "    return df_train, df_test, features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDataSet2():\n",
    "    df_train = pd.read_csv('../data/train/orderFuture_train.csv')\n",
    "    df_test = pd.read_csv('../data/test/orderFuture_test.csv')\n",
    "\n",
    "    df_train = pd.merge(df_train,df_train_1,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_1,how='left',on='userid')\n",
    "   \n",
    "    df_train = pd.merge(df_train,df_train_2,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_2,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_3,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_3,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_4,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_4,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_5,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_5,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_6,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_6,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_7,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_7,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_8,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_8,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_9,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_9,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_10,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_10,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_11,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_11,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_12_top110,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_12_top110,how='left',on='userid')\n",
    "    \n",
    "    goodfeature = list(df_test.columns[1:])\n",
    "\n",
    "    for f in stopfeatures:\n",
    "        if f in goodfeature:\n",
    "            goodfeature.remove(f)\n",
    "    # 设置特征数据，去除id数据，不能进行预测\n",
    "    features = goodfeature\n",
    "\n",
    "    label = 'orderType'\n",
    "    \n",
    "    return df_train, df_test, features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDataSet3():\n",
    "    df_train = pd.read_csv('../data/train/orderFuture_train.csv')\n",
    "    df_test = pd.read_csv('../data/test/orderFuture_test.csv')\n",
    "\n",
    "    df_train = pd.merge(df_train,df_train_1,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_1,how='left',on='userid')\n",
    "   \n",
    "    df_train = pd.merge(df_train,df_train_2,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_2,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_3,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_3,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_4,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_4,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_5,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_5,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_6,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_6,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_7,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_7,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_8,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_8,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_9,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_9,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_10,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_10,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_11,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_11,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_12_top110,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_12_top110,how='left',on='userid')\n",
    "\n",
    "    df_train = pd.merge(df_train,df_train_13,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_13,how='left',on='userid')\n",
    "\n",
    "    \n",
    "    goodfeature = list(df_test.columns[1:])\n",
    "\n",
    "    for f in stopfeatures:\n",
    "        if f in goodfeature:\n",
    "            goodfeature.remove(f)\n",
    "    # 设置特征数据，去除id数据，不能进行预测\n",
    "    features = goodfeature\n",
    "\n",
    "    label = 'orderType'\n",
    "    \n",
    "    return df_train, df_test, features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDataSet4():\n",
    "    df_train = pd.read_csv('../data/train/orderFuture_train.csv')\n",
    "    df_test = pd.read_csv('../data/test/orderFuture_test.csv')\n",
    "\n",
    "    df_train = pd.merge(df_train,df_train_1,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_1,how='left',on='userid')\n",
    "   \n",
    "    df_train = pd.merge(df_train,df_train_2,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_2,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_3,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_3,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_4,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_4,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_5,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_5,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_6,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_6,how='left',on='userid')\n",
    "      \n",
    "    df_train = pd.merge(df_train,df_train_12_top110,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_12_top110,how='left',on='userid')\n",
    "    \n",
    "    goodfeature = list(df_test.columns[1:])\n",
    "\n",
    "    for f in stopfeatures:\n",
    "        if f in goodfeature:\n",
    "            goodfeature.remove(f)\n",
    "    # 设置特征数据，去除id数据，不能进行预测\n",
    "    features = goodfeature\n",
    "\n",
    "    label = 'orderType'\n",
    "    \n",
    "    return df_train, df_test, features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDataSet5():\n",
    "    df_train = pd.read_csv('../data/train/orderFuture_train.csv')\n",
    "    df_test = pd.read_csv('../data/test/orderFuture_test.csv')\n",
    "\n",
    "    df_train = pd.merge(df_train,df_train_1,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_1,how='left',on='userid')\n",
    "   \n",
    "    df_train = pd.merge(df_train,df_train_2,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_2,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_3,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_3,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_4,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_4,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_5,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_5,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_6,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_6,how='left',on='userid')\n",
    "      \n",
    "    df_train = pd.merge(df_train,df_train_12_top110,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_12_top110,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_13,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_13,how='left',on='userid')\n",
    "    \n",
    "    \n",
    "    goodfeature = list(df_test.columns[1:])\n",
    "\n",
    "    for f in stopfeatures:\n",
    "        if f in goodfeature:\n",
    "            goodfeature.remove(f)\n",
    "    # 设置特征数据，去除id数据，不能进行预测\n",
    "    features = goodfeature\n",
    "\n",
    "    label = 'orderType'\n",
    "    \n",
    "    return df_train, df_test, features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDataSet6():\n",
    "    df_train = pd.read_csv('../data/train/orderFuture_train.csv')\n",
    "    df_test = pd.read_csv('../data/test/orderFuture_test.csv')\n",
    "\n",
    "    df_train = pd.merge(df_train,df_train_1,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_1,how='left',on='userid')\n",
    "   \n",
    "    df_train = pd.merge(df_train,df_train_2,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_2,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_3,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_3,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_4,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_4,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_5,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_5,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_6,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_6,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_7,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_7,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_11,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_11,how='left',on='userid')\n",
    "        \n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_12_top110,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_12_top110,how='left',on='userid')\n",
    "    \n",
    "    df_train = pd.merge(df_train,df_train_13,how='left',on='userid')\n",
    "    df_test = pd.merge(df_test,df_test_13,how='left',on='userid')\n",
    "    \n",
    "    \n",
    "    goodfeature = list(df_test.columns[1:])\n",
    "\n",
    "    for f in stopfeatures:\n",
    "        if f in goodfeature:\n",
    "            goodfeature.remove(f)\n",
    "    # 设置特征数据，去除id数据，不能进行预测\n",
    "    features = goodfeature\n",
    "\n",
    "    label = 'orderType'\n",
    "    \n",
    "    return df_train, df_test, features, label\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
